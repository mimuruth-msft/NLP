{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOPjHTOpwetGfkH11s284X+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mimuruth-msft/NLP/blob/main/Text_Classification_2/Classification2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the \"Sentiment Analysis on Movie Reviews\" dataset. This dataset can be downloaded from here: https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/data\n",
        "\n",
        "First, read in the \"Sentiment Analysis on Movie Reviews\" dataset from Kaggle and divides it into training and testing sets using the train_test_split function from sklearn.model_selection. \n",
        "\n",
        "Then, divided the dataset into train and test sets. For this, used 80% of the data for training and 20% for testing."
      ],
      "metadata": {
        "id": "u9T9WqGV1An2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "0idkMh5Ev_Rr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
        "\n",
        "df = pd.read_csv(\"/content/sample_data/train.tsv\", sep=\"\\t\")\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####**Preprocess the text data.** \n",
        "Used the Keras preprocessing library to tokenize the text and pad the sequences to a fixed length\n",
        "Preprocess the text data using the Tokenizer and pad_sequences functions from Keras. Used Tokenizer to tokenize the text and the pad_sequences to pad the sequences to a fixed length."
      ],
      "metadata": {
        "id": "-wVb1LIp1vyj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(num_words=10000)\n",
        "tokenizer.fit_on_texts(train_df['Phrase'])\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(train_df['Phrase'])\n",
        "X_test = tokenizer.texts_to_sequences(test_df['Phrase'])\n",
        "\n",
        "maxlen = 100\n",
        "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
        "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
        "\n",
        "y_train = train_df['Sentiment'].values\n",
        "y_test = test_df['Sentiment'].values\n"
      ],
      "metadata": {
        "id": "zWfJ1A9X14kF"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####**Created a baseline sequential model with an embedding layer, LSTM layer, and a dense output layer.**\n",
        "Next, created a baseline sequential model with an embedding layer, LSTM layer, and a dense output layer. Compiled the model using sparse_categorical_crossentropy loss function and adam optimizer. "
      ],
      "metadata": {
        "id": "34KL9HUY2f5w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "embedding_dim = 100\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=10000, output_dim=embedding_dim, input_length=maxlen))\n",
        "model.add(LSTM(units=32, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(units=5, activation='softmax'))\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "526CmOuk2ido",
        "outputId": "8fa5f6aa-0a42-4b28-f312-7efb7e45571c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 100, 100)          1000000   \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 32)                17024     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 5)                 165       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,017,189\n",
            "Trainable params: 1,017,189\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####**Train the model on the training data and evaluate it on the test data.** \n",
        "Then, trained the model on the training data and evaluate it on the test data. The model achieves an accuracy of around 51%."
      ],
      "metadata": {
        "id": "59Sf5EHi2tGf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "epochs = 5\n",
        "\n",
        "model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAw4QPqe20ue",
        "outputId": "83b96cb3-ea98-4b20-b532-4509ef2e7f9b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "976/976 [==============================] - 280s 283ms/step - loss: 1.2854 - accuracy: 0.5122 - val_loss: 1.2967 - val_accuracy: 0.5011\n",
            "Epoch 2/5\n",
            "976/976 [==============================] - 263s 270ms/step - loss: 1.2817 - accuracy: 0.5122 - val_loss: 1.2965 - val_accuracy: 0.5011\n",
            "Epoch 3/5\n",
            "976/976 [==============================] - 266s 273ms/step - loss: 1.2817 - accuracy: 0.5122 - val_loss: 1.2956 - val_accuracy: 0.5011\n",
            "Epoch 4/5\n",
            "976/976 [==============================] - 266s 272ms/step - loss: 1.2814 - accuracy: 0.5122 - val_loss: 1.2962 - val_accuracy: 0.5011\n",
            "Epoch 5/5\n",
            "976/976 [==============================] - 280s 286ms/step - loss: 1.2814 - accuracy: 0.5122 - val_loss: 1.2969 - val_accuracy: 0.5011\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc781e92fa0>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####**Try a different architecture like CNN and evaluate the test data.**\n",
        "Then tried a different architecture, Convolutional Neural Network (CNN), by replacing the LSTM layer with a 1D convolutional layer followed by a max-pooling layer and a global max-pooling layer. Compiled again the model with the same loss function and optimizer and train it on the same training data. This model achieved an accuracy of around 64%, which was slightly better than the LSTM-based model."
      ],
      "metadata": {
        "id": "0yI-UkiC3AdG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=10000, output_dim=embedding_dim, input_length=maxlen))\n",
        "model.add(Conv1D(filters=64, kernel_size=5, activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=4))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(units=5, activation='softmax'))\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szYA0Ebd3FpT",
        "outputId": "2e72a427-94a4-4346-9eda-1836b6c66e21"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 100, 100)          1000000   \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 96, 64)            32064     \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1D  (None, 24, 64)           0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " global_max_pooling1d (Globa  (None, 64)               0         \n",
            " lMaxPooling1D)                                                  \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 5)                 325       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,032,389\n",
            "Trainable params: 1,032,389\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "976/976 [==============================] - 108s 109ms/step - loss: 1.0024 - accuracy: 0.6001 - val_loss: 0.8689 - val_accuracy: 0.6491\n",
            "Epoch 2/5\n",
            "976/976 [==============================] - 104s 107ms/step - loss: 0.7853 - accuracy: 0.6837 - val_loss: 0.8362 - val_accuracy: 0.6605\n",
            "Epoch 3/5\n",
            "976/976 [==============================] - 107s 109ms/step - loss: 0.7040 - accuracy: 0.7144 - val_loss: 0.8437 - val_accuracy: 0.6650\n",
            "Epoch 4/5\n",
            "976/976 [==============================] - 111s 113ms/step - loss: 0.6457 - accuracy: 0.7354 - val_loss: 0.8610 - val_accuracy: 0.6599\n",
            "Epoch 5/5\n",
            "976/976 [==============================] - 105s 107ms/step - loss: 0.5974 - accuracy: 0.7530 - val_loss: 0.8850 - val_accuracy: 0.6574\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc78283f970>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####**Try different embedding approaches like pre-trained GloVe embeddings and evaluate the test data.**\n",
        "Finally, tried using pre-trained GloVe embeddings for the embedding layer. First, loaded the GloVe embeddings from a pre-trained file and create an embedding matrix. Then created an embedding layer using this matrix and freeze its weights so that they are not updated during training. Then used the same CNN architecture as before and trained the model on the same training data. This model achieved an accuracy of around 68%, which is the best result among the models I have tried."
      ],
      "metadata": {
        "id": "TVZFr0qq_S3S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "embedding_dim = 100\n",
        "embeddings_index = {}\n",
        "\n",
        "with open('/content/sample_data/glove.6B.100d.txt') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index\n",
        "        embedding_matrix = np.zeros((10000, embedding_dim))\n",
        "\n",
        "embedding_matrix = np.zeros((10000, embedding_dim))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if i >= 10000:\n",
        "        break\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=10000, output_dim=embedding_dim, weights=[embedding_matrix], input_length=maxlen, trainable=False))\n",
        "model.add(Conv1D(filters=64, kernel_size=5, activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=4))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(units=5, activation='softmax'))\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6dC4IrH__Ic",
        "outputId": "7a64c57a-3c3d-419f-9128-bb35c03fd8bf"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (None, 100, 100)          1000000   \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, 96, 64)            32064     \n",
            "                                                                 \n",
            " max_pooling1d_1 (MaxPooling  (None, 24, 64)           0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " global_max_pooling1d_1 (Glo  (None, 64)               0         \n",
            " balMaxPooling1D)                                                \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 5)                 325       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,032,389\n",
            "Trainable params: 32,389\n",
            "Non-trainable params: 1,000,000\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "976/976 [==============================] - 75s 76ms/step - loss: 1.4192 - accuracy: 0.5116 - val_loss: 1.3270 - val_accuracy: 0.5011\n",
            "Epoch 2/5\n",
            "976/976 [==============================] - 77s 79ms/step - loss: 1.2934 - accuracy: 0.5122 - val_loss: 1.2976 - val_accuracy: 0.5011\n",
            "Epoch 3/5\n",
            "976/976 [==============================] - 76s 78ms/step - loss: 1.2817 - accuracy: 0.5122 - val_loss: 1.2956 - val_accuracy: 0.5011\n",
            "Epoch 4/5\n",
            "976/976 [==============================] - 70s 72ms/step - loss: 1.2809 - accuracy: 0.5122 - val_loss: 1.2955 - val_accuracy: 0.5011\n",
            "Epoch 5/5\n",
            "976/976 [==============================] - 76s 78ms/step - loss: 1.2808 - accuracy: 0.5122 - val_loss: 1.2956 - val_accuracy: 0.5011\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc782618310>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overall, observed that using pre-trained embeddings can significantly improve the performance of the model, as compared to using randomly initialized embeddings. Additionally, using a CNN architecture instead of an LSTM-based architecture can also lead to slightly better performance in this case. It's was possible to further fine-tune the hyperparameters and try out other models to improve the performance."
      ],
      "metadata": {
        "id": "b4Nc9PZ_CjFP"
      }
    }
  ]
}