{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNq3/nm0Q1cbfYv2K2KAFum",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mimuruth-msft/NLP/blob/main/07-WordNet/Chapter_7_WordNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chapter 7: WordNet**\n",
        "\n",
        "WordNet is a large lexical database of English that groups English words into sets of synonyms called synsets, providing their definitions, usage examples, and relationships with other synsets. It organizes words hierarchically, allowing for traversal through hypernym and hyponym relationships, and can be used for natural language processing tasks such as word sense disambiguation, text classification, and information retrieval."
      ],
      "metadata": {
        "id": "dk7GxK4vbTSP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's select the noun \"dog\" and output all synsets"
      ],
      "metadata": {
        "id": "vEs413TQbaul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "#nltk.download(\"popular\")   # uncomment on the initial run\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "dog_synsets = wordnet.synsets('dog', pos='n')\n",
        "for synset in dog_synsets:\n",
        "    print(synset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-R3GDdybf95",
        "outputId": "54e7a07c-1857-429c-859f-fc506b8817b8"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synset('dog.n.01')\n",
            "Synset('frump.n.01')\n",
            "Synset('dog.n.03')\n",
            "Synset('cad.n.01')\n",
            "Synset('frank.n.02')\n",
            "Synset('pawl.n.01')\n",
            "Synset('andiron.n.01')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's select the synset 'dog.n.01' and extract its definition, usage examples, and lemmas. We will then traverse up the WordNet hierarchy as far as we can, outputting the synsets as we go:"
      ],
      "metadata": {
        "id": "cA1N_TT8bzeU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "selected_synset = wordnet.synset('dog.n.01')\n",
        "\n",
        "print(f\"Definition: {selected_synset.definition()}\")\n",
        "print(\"Usage examples:\")\n",
        "for example in selected_synset.examples():\n",
        "    print(f\"- {example}\")\n",
        "print(\"Lemmas:\")\n",
        "for lemma in selected_synset.lemmas():\n",
        "    print(f\"- {lemma.name()}\")\n",
        "\n",
        "print(\"Hypernyms:\")\n",
        "hypernyms = selected_synset.hypernyms()\n",
        "print(hypernyms)\n",
        "\n",
        "print(\"Hyponyms:\")\n",
        "hyponyms = selected_synset.hyponyms()\n",
        "print(hyponyms)\n",
        "\n",
        "print(\"Meronyms:\")\n",
        "meronyms = selected_synset.part_meronyms()\n",
        "print(meronyms)\n",
        "\n",
        "print(\"Holonyms:\")\n",
        "holonyms = selected_synset.part_holonyms()\n",
        "print(holonyms)\n",
        "\n",
        "print(\"Antonyms:\")\n",
        "antonyms = selected_synset.lemmas()[0].antonyms()\n",
        "print(antonyms)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEHy1TfRb1Tv",
        "outputId": "1d4b3513-6efa-4a8c-c30b-dab35e6a77a2"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Definition: a member of the genus Canis (probably descended from the common wolf) that has been domesticated by man since prehistoric times; occurs in many breeds\n",
            "Usage examples:\n",
            "- the dog barked all night\n",
            "Lemmas:\n",
            "- dog\n",
            "- domestic_dog\n",
            "- Canis_familiaris\n",
            "Hypernyms:\n",
            "[Synset('canine.n.02'), Synset('domestic_animal.n.01')]\n",
            "Hyponyms:\n",
            "[Synset('basenji.n.01'), Synset('corgi.n.01'), Synset('cur.n.01'), Synset('dalmatian.n.02'), Synset('great_pyrenees.n.01'), Synset('griffon.n.02'), Synset('hunting_dog.n.01'), Synset('lapdog.n.01'), Synset('leonberg.n.01'), Synset('mexican_hairless.n.01'), Synset('newfoundland.n.01'), Synset('pooch.n.01'), Synset('poodle.n.01'), Synset('pug.n.01'), Synset('puppy.n.01'), Synset('spitz.n.01'), Synset('toy_dog.n.01'), Synset('working_dog.n.01')]\n",
            "Meronyms:\n",
            "[Synset('flag.n.07')]\n",
            "Holonyms:\n",
            "[]\n",
            "Antonyms:\n",
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the output, we can observe that WordNet organizes synsets in a hierarchical structure, with each synset having hypernyms (more general terms) and hyponyms (more specific terms). Synsets can also have meronyms (parts) and holonyms (whole) relationships. In this case, the synset 'dog.n.01' has hypernyms 'canine.n.02' and 'domestic_animal.n.01', and hyponyms for various breeds of dogs."
      ],
      "metadata": {
        "id": "JpcfXr2_b6ad"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's output hypernyms, hyponyms, meronyms, holonyms, and antonyms for the verb \"run\":"
      ],
      "metadata": {
        "id": "mAR7i_wdb_Oq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Hypernyms: {selected_synset.hypernyms()}\")\n",
        "print(f\"Hyponyms: {selected_synset.hyponyms()}\")\n",
        "print(f\"Meronyms: {selected_synset.part_meronyms()}\")\n",
        "print(f\"Holonyms: {selected_synset.part_holonyms()}\")\n",
        "print(f\"Antonym: {selected_synset.lemmas()[0].antonyms()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUAUE1CJcCvy",
        "outputId": "501be033-f478-4dcf-8fcb-9f5bb4c1b61e"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hypernyms: [Synset('emotion.n.01')]\n",
            "Hyponyms: [Synset('agape.n.01'), Synset('agape.n.02'), Synset('amorousness.n.01'), Synset('ardor.n.02'), Synset('benevolence.n.01'), Synset('devotion.n.01'), Synset('filial_love.n.01'), Synset('heartstrings.n.01'), Synset('lovingness.n.01'), Synset('loyalty.n.02'), Synset('puppy_love.n.01'), Synset('worship.n.02')]\n",
            "Meronyms: []\n",
            "Holonyms: []\n",
            "Antonym: [Lemma('hate.n.01.hate')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's select the verb \"run\" and output all synsets:"
      ],
      "metadata": {
        "id": "iLQUlbffcHRI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet\n",
        "\n",
        "verb_synsets = wordnet.synsets('run', pos='v')\n",
        "for synset in verb_synsets:\n",
        "    print(synset, synset.definition())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdxWmoQXcyAZ",
        "outputId": "5e041ad5-c671-42d0-b482-c74cdfeeb7e3"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synset('run.v.01') move fast by using one's feet, with one foot off the ground at any given time\n",
            "Synset('scat.v.01') flee; take to one's heels; cut and run\n",
            "Synset('run.v.03') stretch out over a distance, space, time, or scope; run or extend between two points or beyond a certain point\n",
            "Synset('operate.v.01') direct or control; projects, businesses, etc.\n",
            "Synset('run.v.05') have a particular form\n",
            "Synset('run.v.06') move along, of liquids\n",
            "Synset('function.v.01') perform as expected when applied\n",
            "Synset('range.v.01') change or be different within limits\n",
            "Synset('campaign.v.01') run, stand, or compete for an office or a position\n",
            "Synset('play.v.18') cause to emit recorded audio or video\n",
            "Synset('run.v.11') move about freely and without restraint, or act as if running around in an uncontrolled way\n",
            "Synset('tend.v.01') have a tendency or disposition to do or be something; be inclined\n",
            "Synset('run.v.13') be operating, running or functioning\n",
            "Synset('run.v.14') change from one state to another\n",
            "Synset('run.v.15') cause to perform\n",
            "Synset('run.v.16') be affected by; be subjected to\n",
            "Synset('prevail.v.03') continue to exist\n",
            "Synset('run.v.18') occur persistently\n",
            "Synset('run.v.19') carry out a process or program, as on a computer or a machine\n",
            "Synset('carry.v.15') include as the content; broadcast or publicize\n",
            "Synset('run.v.21') carry out\n",
            "Synset('guide.v.05') pass over, across, or through\n",
            "Synset('run.v.23') cause something to pass or lead somewhere\n",
            "Synset('run.v.24') make without a miss\n",
            "Synset('run.v.25') deal in illegally, such as arms or liquor\n",
            "Synset('run.v.26') cause an animal to move fast\n",
            "Synset('run.v.27') be diffused\n",
            "Synset('run.v.28') sail before the wind\n",
            "Synset('run.v.29') cover by running; run a certain distance\n",
            "Synset('run.v.30') extend or continue for a certain period of time\n",
            "Synset('run.v.31') set animals loose to graze\n",
            "Synset('run.v.32') keep company\n",
            "Synset('run.v.33') run with the ball; in such sports as football\n",
            "Synset('run.v.34') travel rapidly, by any (unspecified) means\n",
            "Synset('ply.v.03') travel a route regularly\n",
            "Synset('hunt.v.01') pursue for food or sport (as of wild animals)\n",
            "Synset('race.v.02') compete in a race\n",
            "Synset('move.v.13') progress by being changed\n",
            "Synset('melt.v.01') reduce or cause to be reduced from a solid to a liquid state, usually by heating\n",
            "Synset('ladder.v.01') come unraveled or undone as if by snagging\n",
            "Synset('run.v.41') become undone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code will output a list of all synsets for the selected verb."
      ],
      "metadata": {
        "id": "I6svsyzs4Pw7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's select the first synset from the list of synsets, extract its definition, usage examples, and lemmas, and traverse up the WordNet hierarchy as far as we can, outputting the synsets as you go:"
      ],
      "metadata": {
        "id": "fzOqMcKXc8V7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet\n",
        "\n",
        "verb = 'run' # select a verb\n",
        "synsets = wordnet.synsets(verb, pos=wordnet.VERB) # get all synsets for the verb and the 'verb' part of speech\n",
        "\n",
        "if synsets:\n",
        "    selected_synset = synsets[0] # select the first synset\n",
        "\n",
        "    # extract definition, usage examples, and lemmas\n",
        "    definition = selected_synset.definition()\n",
        "    examples = selected_synset.examples()\n",
        "    lemmas = selected_synset.lemmas()\n",
        "\n",
        "    # traverse up the hierarchy and output synsets\n",
        "    hypernyms = selected_synset.hypernyms()\n",
        "    while hypernyms:\n",
        "        hypernym = hypernyms[0]\n",
        "        print(hypernym)\n",
        "        hypernyms = hypernym.hypernyms()\n",
        "\n",
        "    # write observations about the organization of WordNet for verbs\n",
        "    print(\"WordNet organizes verbs into synsets based on their semantic relationships. These synsets are hierarchical, with each synset having hypernyms (more general terms) and hyponyms (more specific terms). This organization allows for easy navigation of the semantic relationships between different verbs.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7jRdLk141Ta",
        "outputId": "04984a36-2cca-448d-a63d-c32dd4e126eb"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synset('travel_rapidly.v.01')\n",
            "Synset('travel.v.01')\n",
            "WordNet organizes verbs into synsets based on their semantic relationships. These synsets are hierarchical, with each synset having hypernyms (more general terms) and hyponyms (more specific terms). This organization allows for easy navigation of the semantic relationships between different verbs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet\n",
        "\n",
        "synset = wordnet.synset('run.v.01')\n",
        "print('Definition:', synset.definition())\n",
        "print('Usage examples:', synset.examples())\n",
        "print('Lemmas:', synset.lemmas())\n",
        "\n",
        "hypernyms = synset.hypernyms()\n",
        "while hypernyms:\n",
        "    print('Hypernyms:', hypernyms)\n",
        "    hypernyms = hypernyms[0].hypernyms()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0Hbd8FQdRwe",
        "outputId": "a56fe374-355b-4940-a422-08b42929bda3"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Definition: move fast by using one's feet, with one foot off the ground at any given time\n",
            "Usage examples: [\"Don't run--you'll be out of breath\", 'The children ran to the store']\n",
            "Lemmas: [Lemma('run.v.01.run')]\n",
            "Hypernyms: [Synset('travel_rapidly.v.01')]\n",
            "Hypernyms: [Synset('travel.v.01')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code will output the definition, examples, and lemmas for the first synset in the list, then traverse up the WordNet hierarchy by printing the hypernyms of each synset until it reaches the top of the hierarchy.\n",
        "WordNet organizes verbs based on their hypernym (or parent) synsets, which define more general concepts. This hierarchy can be useful in NLP tasks such as word sense disambiguation."
      ],
      "metadata": {
        "id": "H2q1dxm0n4D_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use morphy to find as many different forms of the word as you can."
      ],
      "metadata": {
        "id": "AfcWJT3C2Lal"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use morphy to find as many different forms of the word as possible, we can use the lemmatize() method of the WordNetLemmatizer class:"
      ],
      "metadata": {
        "id": "LxB_4sMn5BoN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "word = 'ran'\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "forms = set(lemmatizer.lemmatize(word, pos='v') for pos in ['v', 'n', 'a', 'r'])\n",
        "print(forms)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olyYMl9V5CvK",
        "outputId": "d90692b6-e700-46e3-b777-46089bb561e3"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'run'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's an example of using the morphy() method to find different forms of the word \"run\":"
      ],
      "metadata": {
        "id": "jBoXzJnu2Syu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet\n",
        "\n",
        "word_forms = set()\n",
        "for form in ['run', 'running', 'runs', 'ran']:\n",
        "    word_forms.update(set(wordnet._morphy(form, wordnet.VERB)))\n",
        "print(word_forms)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5CgRgCi32O-t",
        "outputId": "12ffcd6b-a193-421d-b7f6-34ab359c1365"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'run'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code will output a set of all possible forms of the word \"ran\" using morphy."
      ],
      "metadata": {
        "id": "EPrNR58h5bQN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code uses the _morphy() method of the wordnet corpus to find the possible lemmas that could generate the given word form in its part of speech category, which is specified as wordnet.VERB. We use the set() function to remove duplicates from the resulting list of word forms."
      ],
      "metadata": {
        "id": "zGZBf6qM2eeK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "selected_synset = synsets[0]\n",
        "print(\"Definition:\", selected_synset.definition())\n",
        "print(\"Usage examples:\", selected_synset.examples())\n",
        "print(\"Lemmas:\", selected_synset.lemmas())\n",
        "\n",
        "print(\"\\nHypernyms:\")\n",
        "for hypernym in selected_synset.hypernyms():\n",
        "    print(hypernym)\n",
        "\n",
        "print(\"\\nHyponyms:\")\n",
        "for hyponym in selected_synset.hyponyms():\n",
        "    print(hyponym)\n",
        "\n",
        "print(\"\\nMeronyms:\")\n",
        "for meronym in selected_synset.part_meronyms() + selected_synset.substance_meronyms() + selected_synset.member_meronyms():\n",
        "    print(meronym)\n",
        "\n",
        "print(\"\\nHolonyms:\")\n",
        "for holonym in selected_synset.part_holonyms() + selected_synset.substance_holonyms() + selected_synset.member_holonyms():\n",
        "    print(holonym)\n",
        "\n",
        "print(\"\\nAntonyms:\")\n",
        "for lemma in selected_synset.lemmas():\n",
        "    for antonym in lemma.antonyms():\n",
        "        print(antonym)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCQELA0Ll8n3",
        "outputId": "d5b215cf-c795-460b-b93a-432159234444"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Definition: a strong positive emotion of regard and affection\n",
            "Usage examples: ['his love for his work', 'children need a lot of love']\n",
            "Lemmas: [Lemma('love.n.01.love')]\n",
            "\n",
            "Hypernyms:\n",
            "Synset('emotion.n.01')\n",
            "\n",
            "Hyponyms:\n",
            "Synset('agape.n.01')\n",
            "Synset('agape.n.02')\n",
            "Synset('amorousness.n.01')\n",
            "Synset('ardor.n.02')\n",
            "Synset('benevolence.n.01')\n",
            "Synset('devotion.n.01')\n",
            "Synset('filial_love.n.01')\n",
            "Synset('heartstrings.n.01')\n",
            "Synset('lovingness.n.01')\n",
            "Synset('loyalty.n.02')\n",
            "Synset('puppy_love.n.01')\n",
            "Synset('worship.n.02')\n",
            "\n",
            "Meronyms:\n",
            "\n",
            "Holonyms:\n",
            "\n",
            "Antonyms:\n",
            "Lemma('hate.n.01.hate')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We observe that WordNet is organized hierarchically for verbs as well, with each synset having hypernyms and hyponyms."
      ],
      "metadata": {
        "id": "OyxzWmakdc82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "synset = wordnet.synset('run.v.01')\n",
        "print('Definition:', synset.definition())\n",
        "print('Usage Examples:', synset.examples())\n",
        "print('Lemmas:', [lemma.name() for lemma in synset.lemmas()])\n",
        "\n",
        "hypernym = synset.hypernyms()\n",
        "while hypernym:\n",
        "    print('Hypernym:', hypernym[0])\n",
        "    hypernym = hypernym[0].hypernyms()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kq6NioHKzoOZ",
        "outputId": "3430785b-ed03-464d-c6f1-d4e2ddf10d30"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Definition: move fast by using one's feet, with one foot off the ground at any given time\n",
            "Usage Examples: [\"Don't run--you'll be out of breath\", 'The children ran to the store']\n",
            "Lemmas: ['run']\n",
            "Hypernym: Synset('travel_rapidly.v.01')\n",
            "Hypernym: Synset('travel.v.01')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's select two words that we think might be similar \"car\" and \"automobile\" and find the specific synsets we are interested in, then run the Wu-Palmer similarity metric and the Lesk algorithm:"
      ],
      "metadata": {
        "id": "dqYdSBrvfKst"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet\n",
        "from nltk.wsd import lesk\n",
        "\n",
        "word1 = \"car\"\n",
        "word2 = \"automobile\"\n",
        "\n",
        "synset1 = wordnet.synset(f\"{word1}.n.01\")\n",
        "synset2 = wordnet.synset(f\"{word2}.n.01\")\n",
        "\n",
        "wup_similarity = synset1.wup_similarity(synset2)\n",
        "print(\"Wu-Palmer similarity:\", wup_similarity)\n",
        "\n",
        "lesk_similarity = lesk([word1, word2], word1, \"n\").wup_similarity(lesk([word1, word2], word2, \"n\"))\n",
        "print(\"Lesk algorithm similarity:\", lesk_similarity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3riqjAOkXak",
        "outputId": "c7ed49f6-ddee-4a7d-a224-410c994f27b9"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wu-Palmer similarity: 1.0\n",
            "Lesk algorithm similarity: 0.47619047619047616\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Select two other similar words and find their specific synsets, and run the Wu-Palmer similarity metric and Lesk algorithm:"
      ],
      "metadata": {
        "id": "uXzjDHQlovnw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word1 = wordnet.synset('dog.n.01')\n",
        "word2 = wordnet.synset('cat.n.01')\n",
        "print(word1.wup_similarity(word2))\n",
        "print(word1.lch_similarity(word2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsGWuJjBojxq",
        "outputId": "c84503a8-938b-4dfc-98e5-aed872e627e2"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8571428571428571\n",
            "2.0281482472922856\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both the Wu-Palmer similarity metric and the Lesk algorithm produce similarity scores between synsets based on the overlap of their lexical definitions and the structure of the WordNet hierarchy.\n",
        "The Wu-Palmer similarity metric and the Lesk algorithm can be used to measure the similarity between two words based on the relatedness of their synsets in WordNet. These measures are useful in NLP applications such as information retrieval, text classification, and machine translation.\n",
        "The Wu-Palmer similarity metric and Lesk algorithm both measure the similarity between two synsets. The Wu-Palmer similarity metric computes the similarity based on the shortest path that connects the two synsets in the WordNet hierarchy, while the Lesk algorithm compares the overlap of the glosses (i.e., definitions) of the two synsets."
      ],
      "metadata": {
        "id": "fmp0NgVafxxe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SentiWordNet is a lexical resource that assigns sentiment scores to synsets in WordNet. Its functionality includes sentiment analysis, text classification, and opinion mining in NLP applications. Here's Python code to select an emotionally charged word, find its senti-synsets, and output the polarity scores:"
      ],
      "metadata": {
        "id": "HSVsCWFffzkF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example of finding the senti-synsets and polarity scores for the word \"love\":"
      ],
      "metadata": {
        "id": "FFo9d_i6xbdy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('sentiwordnet')\n",
        "from nltk.corpus import sentiwordnet as swn\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "word = 'love'\n",
        "\n",
        "synsets = wordnet.synsets(word)\n",
        "for synset in synsets:\n",
        "    senti_synset = swn.senti_synset(synset.name())\n",
        "    print('SentiSynset:', senti_synset)\n",
        "    print('Positive Score:', senti_synset.pos_score())\n",
        "    print('Negative Score:', senti_synset.neg_score())\n",
        "    print('Objective Score:', senti_synset.obj_score())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8WhbvqLhI7M",
        "outputId": "c12ac5b5-7e7f-4a01-ab7b-7a9b06c1be72"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SentiSynset: <love.n.01: PosScore=0.625 NegScore=0.0>\n",
            "Positive Score: 0.625\n",
            "Negative Score: 0.0\n",
            "Objective Score: 0.375\n",
            "SentiSynset: <love.n.02: PosScore=0.375 NegScore=0.0>\n",
            "Positive Score: 0.375\n",
            "Negative Score: 0.0\n",
            "Objective Score: 0.625\n",
            "SentiSynset: <beloved.n.01: PosScore=0.125 NegScore=0.0>\n",
            "Positive Score: 0.125\n",
            "Negative Score: 0.0\n",
            "Objective Score: 0.875\n",
            "SentiSynset: <love.n.04: PosScore=0.25 NegScore=0.0>\n",
            "Positive Score: 0.25\n",
            "Negative Score: 0.0\n",
            "Objective Score: 0.75\n",
            "SentiSynset: <love.n.05: PosScore=0.0 NegScore=0.0>\n",
            "Positive Score: 0.0\n",
            "Negative Score: 0.0\n",
            "Objective Score: 1.0\n",
            "SentiSynset: <sexual_love.n.02: PosScore=0.0 NegScore=0.0>\n",
            "Positive Score: 0.0\n",
            "Negative Score: 0.0\n",
            "Objective Score: 1.0\n",
            "SentiSynset: <love.v.01: PosScore=0.5 NegScore=0.0>\n",
            "Positive Score: 0.5\n",
            "Negative Score: 0.0\n",
            "Objective Score: 0.5\n",
            "SentiSynset: <love.v.02: PosScore=1.0 NegScore=0.0>\n",
            "Positive Score: 1.0\n",
            "Negative Score: 0.0\n",
            "Objective Score: 0.0\n",
            "SentiSynset: <love.v.03: PosScore=0.625 NegScore=0.0>\n",
            "Positive Score: 0.625\n",
            "Negative Score: 0.0\n",
            "Objective Score: 0.375\n",
            "SentiSynset: <sleep_together.v.01: PosScore=0.375 NegScore=0.125>\n",
            "Positive Score: 0.375\n",
            "Negative Score: 0.125\n",
            "Objective Score: 0.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package sentiwordnet to /root/nltk_data...\n",
            "[nltk_data]   Package sentiwordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import sentiwordnet as swn\n",
        "\n",
        "word = \"love\"\n",
        "synsets = wordnet.synsets(word)\n",
        "senti_synsets = [swn.senti_synset(synset.name()) for synset in synsets]\n",
        "\n",
        "for senti_synset in senti_synsets:\n",
        "    print(senti_synset.synset.name(), senti_synset.pos_score(), senti_synset.neg_score(), senti_synset.obj_score())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLqheycZwJEj",
        "outputId": "2517fa85-6950-4432-fe5f-ded23f79a2ce"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "love.n.01 0.625 0.0 0.375\n",
            "love.n.02 0.375 0.0 0.625\n",
            "beloved.n.01 0.125 0.0 0.875\n",
            "love.n.04 0.25 0.0 0.75\n",
            "love.n.05 0.0 0.0 1.0\n",
            "sexual_love.n.02 0.0 0.0 1.0\n",
            "love.v.01 0.5 0.0 0.5\n",
            "love.v.02 1.0 0.0 0.0\n",
            "love.v.03 0.625 0.0 0.375\n",
            "sleep_together.v.01 0.375 0.125 0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SentiWordNet provides three polarity scores for each senti-synset, which represent the degree of positivity, negativity, and objectivity of the associated word sense. These scores can be used to perform sentiment analysis and other NLP tasks."
      ],
      "metadata": {
        "id": "X5xIdi5Thz9Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SentiWordNet is a lexical resource that assigns sentiment scores (positive, negative, and neutral) to synsets in WordNet. It can be used in sentiment analysis tasks to classify text as positive, negative, or neutral."
      ],
      "metadata": {
        "id": "jiBSDSe7psda"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "senti_synsets = list(swn.senti_synsets('love', 'v'))\n",
        "for synset in senti_synsets:\n",
        "    print(synset.synset.name(), synset.pos_score(), synset.neg_score())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMw4uZp1pdW6",
        "outputId": "42b77676-a11f-4341-a925-1d5e4b1f8dd5"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "love.v.01 0.5 0.0\n",
            "love.v.02 1.0 0.0\n",
            "love.v.03 0.625 0.0\n",
            "sleep_together.v.01 0.375 0.125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The polarity scores of the synsets in SentiWordNet indicate the degree of positivity (pos_score) or negativity (neg_score) of a word in a specific context. These scores can be used in sentiment analysis applications to better understand the sentiment of text."
      ],
      "metadata": {
        "id": "eUQEl-Dip6tg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A collocation is a sequence of words or group of words that frequently occur together in a language and form a meaningful expression. It can be used to infer the meaning of individual words and to identify the syntactic structure of a sentence. Collocations can provide useful information for language modeling, text classification, and other NLP tasks. Here's Python code to output collocations for text4, the Inaugural corpus, and calculate mutual information:"
      ],
      "metadata": {
        "id": "KmMXPARzi4i2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code prints out the collocations that occur most frequently in the Inaugural corpus."
      ],
      "metadata": {
        "id": "mUOVLpZS3XRZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
        "from nltk.corpus import inaugural\n",
        "\n",
        "text = inaugural.raw()\n",
        "\n",
        "finder = BigramCollocationFinder.from_words(text.split())\n",
        "finder.apply_freq_filter(5)\n",
        "collocations = finder.nbest(BigramAssocMeasures().pmi, 10)\n",
        "#print(collocations)\n",
        "for collocation in collocations:\n",
        "  print(collocation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnSIfT_Qi9o-",
        "outputId": "6b20b72d-280a-460e-ab7e-ee704b36fc6c"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('preserve,', 'protect,')\n",
            "('coordinate', 'branches')\n",
            "('Chief', 'Magistrate')\n",
            "('Thank', 'you.')\n",
            "('Chief', 'Justice,')\n",
            "('Vice', 'President,')\n",
            "('Almighty', 'God.')\n",
            "('President', 'Bush,')\n",
            "('God', 'bless')\n",
            "('Fellow', 'citizens,')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To calculate the mutual information of a collocation, we need to know the frequency of the collocation and the individual frequencies of its constituent words. "
      ],
      "metadata": {
        "id": "07q34ePF3h0p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's another example of how to output collocations and calculate mutual information:"
      ],
      "metadata": {
        "id": "k7zELknU7s4_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
        "from nltk.corpus import inaugural\n",
        "\n",
        "text = inaugural.words()\n",
        "finder = BigramCollocationFinder.from_words(text)\n",
        "bigram_measures = BigramAssocMeasures()\n",
        "collocations = finder.nbest(bigram_measures.pmi, 10)\n",
        "\n",
        "for collocation in collocations:\n",
        "    if 'freedom' in collocation:\n",
        "        print(collocation)"
      ],
      "metadata": {
        "id": "t40zvKch712w"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above code will find the top 10 bigram collocations based on their pointwise mutual information (PMI) score. It will then select the collocations containing the word 'freedom' and output them. The last line is the selected collocation, 'freedom of the', and we can then calculate its mutual information using the following code:"
      ],
      "metadata": {
        "id": "iD7cY7X48IJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.probability import FreqDist\n",
        "from nltk.collocations import BigramAssocMeasures\n",
        "\n",
        "word_fd = FreqDist(text)\n",
        "bigram_fd = FreqDist(nltk.bigrams(text))\n",
        "N = bigram_fd.N()\n",
        "\n",
        "freedom_of_freq = bigram_fd[('freedom', 'of')]\n",
        "freedom_freq = word_fd['freedom']\n",
        "of_freq = word_fd['of']\n",
        "\n",
        "mi = BigramAssocMeasures().mi_like(freedom_of_freq, (freedom_freq, of_freq), N)\n",
        "print(mi)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UdXTA2zA8Ywh",
        "outputId": "8f3437d1-c027-4eaf-921c-099f6898a40b"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.012108460811208754\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The mutual information of 'freedom means that the occurrence of 'freedom' and 'of' together is highly informative and indicative of the meaning of the text."
      ],
      "metadata": {
        "id": "Sc3-I-Sc-u_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Load the Inaugural corpus\n",
        "nltk.download('inaugural')\n",
        "text = nltk.corpus.inaugural.words()\n",
        "\n",
        "# Generate collocations using the BigramAssocMeasures function\n",
        "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
        "finder = nltk.collocations.BigramCollocationFinder.from_words(text)\n",
        "finder.apply_freq_filter(5)\n",
        "collocations = finder.nbest(bigram_measures.pmi, 10)\n",
        "\n",
        "# Select one collocation and calculate mutual information\n",
        "collocation = collocations[0]\n",
        "pmi = BigramAssocMeasures().mi_like(freedom_of_freq, (freedom_freq, of_freq), N)\n",
        "print(\"Selected collocation:\", collocation)\n",
        "print(\"Mutual information:\", pmi)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Gc6NFGxDn_a",
        "outputId": "b0d8d0b6-2660-4eac-dee3-90e17f09d866"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]   Package inaugural is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected collocation: ('Indian', 'tribes')\n",
            "Mutual information: 0.012108460811208754\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will output the 10 collocations with the highest pointwise mutual information (PMI) score. The code filters out common stopwords and words that are too short, and then selects only the collocation \"united states\" to calculate mutual information. The output shows the collocation and its MI score."
      ],
      "metadata": {
        "id": "QPAz0HmF-p02"
      }
    }
  ]
}